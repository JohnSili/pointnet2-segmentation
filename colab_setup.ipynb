{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PointNet++ для семантической сегментации облака точек\n",
        "\n",
        "Этот ноутбук настраивает окружение и запускает обучение модели PointNet++\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Установка зависимостей\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка всех необходимых библиотек\n",
        "%pip install torch torchvision numpy scikit-learn tqdm matplotlib tensorboard -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Клонирование репозитория или загрузка файлов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Вариант 1: Клонировать из GitHub (замените YOUR_USERNAME)\n",
        "# !git clone https://github.com/YOUR_USERNAME/pointnet2-segmentation.git\n",
        "# %cd pointnet2-segmentation\n",
        "\n",
        "# Вариант 2: Загрузить файлы через вкладку Files в Colab\n",
        "# Затем раскомментируйте:\n",
        "# %cd /content/pointnet2-segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Загрузка данных\n",
        "\n",
        "**Вариант A: Загрузить ваши PLY файлы (рекомендуется)**\n",
        "\n",
        "У вас есть 500 PLY файлов (~85MB). Загрузите архив с данными:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Загрузите архив с данными (создайте его локально: ./prepare_data_for_colab.sh)\n",
        "print(\"Загрузите архив data_for_colab.zip с вашими PLY файлами\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Распакуйте архив\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        print(f\"Распаковка {filename}...\")\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "        print(f\"✓ Данные распакованы!\")\n",
        "        break\n",
        "\n",
        "# Проверка наличия данных\n",
        "data_dir = '3011-20251217T195928Z-1-001'\n",
        "area = '3011'\n",
        "if os.path.exists(os.path.join(data_dir, area)):\n",
        "    ply_files = [f for f in os.listdir(os.path.join(data_dir, area)) if f.endswith('.ply')]\n",
        "    print(f\"✓ Найдено {len(ply_files)} PLY файлов\")\n",
        "else:\n",
        "    print(\"⚠ Данные не найдены. Будем использовать синтетические данные.\")\n",
        "    data_dir = '.'\n",
        "    area = 'synthetic'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Проверка данных\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset import S3DISDataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Автоматическое определение пути к данным\n",
        "if os.path.exists('3011-20251217T195928Z-1-001/3011'):\n",
        "    data_dir = '3011-20251217T195928Z-1-001'\n",
        "    area = '3011'\n",
        "    print(\"✓ Используем реальные данные из 3011-20251217T195928Z-1-001/3011\")\n",
        "else:\n",
        "    data_dir = '.'\n",
        "    area = 'synthetic'\n",
        "    print(\"⚠ Реальные данные не найдены. Используем синтетические данные.\")\n",
        "\n",
        "# Проверка данных\n",
        "dataset = S3DISDataset(data_dir, area=area, split='train', num_points=2048)\n",
        "print(f'\\nDataset size: {len(dataset)} файлов')\n",
        "points, labels = dataset[0]\n",
        "print(f'Points shape: {points.shape}, Labels shape: {labels.shape}')\n",
        "print(f'Unique classes: {sorted(torch.unique(labels).tolist())}')\n",
        "\n",
        "# Сохраняем для использования в обучении\n",
        "DATA_DIR = data_dir\n",
        "AREA = area\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Обучение модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Запуск обучения с вашими данными\n",
        "# Если данные загружены, используйте:\n",
        "#   --data_dir 3011-20251217T195928Z-1-001 --area 3011\n",
        "# Если данных нет, используйте синтетические:\n",
        "#   --data_dir . --area synthetic\n",
        "\n",
        "!python train.py \\\n",
        "    --data_dir {DATA_DIR} \\\n",
        "    --area {AREA} \\\n",
        "    --num_points 2048 \\\n",
        "    --batch_size 8 \\\n",
        "    --epochs 50 \\\n",
        "    --lr 0.001 \\\n",
        "    --num_classes 13 \\\n",
        "    --device cuda \\\n",
        "    --save_dir ./checkpoints \\\n",
        "    --log_dir ./logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Мониторинг обучения (TensorBoard)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Запуск TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Тестирование модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python test.py \\\n",
        "    --data_dir {DATA_DIR} \\\n",
        "    --area {AREA} \\\n",
        "    --checkpoint ./checkpoints/best_model.pth \\\n",
        "    --num_classes 13\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Визуализация результатов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python visualize.py \\\n",
        "    --data_dir {DATA_DIR} \\\n",
        "    --area {AREA} \\\n",
        "    --checkpoint ./checkpoints/best_model.pth \\\n",
        "    --num_samples 5 \\\n",
        "    --output_dir ./visualizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Скачивание результатов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Создаем архив с результатами\n",
        "with zipfile.ZipFile('results.zip', 'w') as zipf:\n",
        "    if os.path.exists('checkpoints'):\n",
        "        for root, dirs, files_list in os.walk('checkpoints'):\n",
        "            for file in files_list:\n",
        "                zipf.write(os.path.join(root, file))\n",
        "    if os.path.exists('visualizations'):\n",
        "        for root, dirs, files_list in os.walk('visualizations'):\n",
        "            for file in files_list:\n",
        "                zipf.write(os.path.join(root, file))\n",
        "\n",
        "# Скачиваем архив\n",
        "files.download('results.zip')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
